<!DOCTYPE HTML><html><head><meta charset="utf-8"><meta name="viewport" content="with=device-width, initial-scale=1.0"><title>Lucas DiCioccio&#39;s blog - Optimal Lab Samples Spreading</title><meta name="author" content="Lucas DiCioccio"><meta name="keywords" content="minizinc, optimization"><meta name="description" content="An illustration of how to model a constraint-problem to optimally distribute testing samples to laboratories."><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Lucas DiCioccio&#39;s blog - Optimal Lab Samples Spreading"><meta name="twitter:site" content="@lucasdicioccio"><meta name="twitter:creator" content="lucasdicioccio"><meta property="twitter:description" content="An illustration of how to model a constraint-problem to optimally distribute testing samples to laboratories."><meta name="type" property="og:type" content="article"><meta name="title" property="og:title" content="Lucas DiCioccio&#39;s blog - Optimal Lab Samples Spreading"><meta name="url" property="og:url" content="https://lucasdicioccio.github.io/optimal-lab-samples-spreading.html"><meta property="og:description" content="An illustration of how to model a constraint-problem to optimally distribute testing samples to laboratories."><meta name="article:published_time" content="2022-02-20"><meta name="article:tag" content="minizinc"><meta name="article:tag" content="optimization"><style>@import "/css/main.css";</style></head><body><aside class="nav"><a href="/">home</a></aside><div class="main"><article><header class="heading"><h1>Optimal Lab Samples Spreading</h1><p>On <span>Sun, 20 Feb 2022</span>, by <a href="https://twitter.com/lucasdicioccio">@lucasdicioccio</a>, 2623 words.</p><div class="taglist"><div class="tag"><a class="tag-link" href="/topics/minizinc.html"><span class="tag-name">minizinc</span><span class="tag-count">4</span></a></div><div class="tag"><a class="tag-link" href="/topics/optimization.html"><span class="tag-name">optimization</span><span class="tag-count">3</span></a></div></div></header><div class="upcoming-notice"><p>This article is still considered unfinished and content may change significantly.</p></div><div class="main-article"><section class="main-section"><p>This article illustrates what the work of modeling a problem for constraint
optimization entails.</p>
<h1>A problem with a vague description</h1>
<p>Unless you’ve been living under a rock, you are aware that the COVID19 pandemic
happened and is still ongoing. One thing that has become common at least in
rich countries is the frequent tests we need to perform to validate whether a
person is infected or not. A variety of test technologies have different
characteristics, some tests can be done at home (rapid anti-genic tests).
Whereas PCR tests need a specific person to carry the <em>sample</em> then the samples
are sent to a <em>laboratory</em> that processes the PCR check.</p>
<p>Now let’s say there is some imbalance between PCR-check laboratories and some
tests sites. A question that may arise is: how do we best allocate samples from
various sampling <em>locations</em> (e.g., pharmacies) to laboratories? There is some
delay in sending tests to laboratories, and also there is a maximum delay to
wait for a sample (otherwise the sample deteriorates and can no longer provide
good material for the test). What do we do for a day, over a week?</p>
<h1>An off-the-shelf model</h1>
<p>Before using a modeling approach solver, it’s always good to verify whether the
problem has an easy-to-recognize shape. This instance looks like an <a href="https://en.wikipedia.org/wiki/Maximum_flow_problem">max-flow
problem</a> where testing
locations are sources and laboratories are samples: indeed maximizing the
number of samples that go through testing is a way to minize those which are
thrown out.</p>
<p>The following example shows a possible max-flow model to this problem. On the
left you have a fictional <code>demand</code> node and on the right a fictional
<code>processed</code> node. The <code>from</code> arrows are capacities which corresponds to the
demand at each sample-collection location. The symmetric side of processing-
capacities are <code>to</code> arrows. The center arrows with <code>?</code> are the attribution
matrix. We could force some capacities to zero to say a sample-location is too
far from a certain site, or leave them infinite to express that any amount of
samples can be sent.</p>
<p><img src="/gen/images/labspread-maxflow.dot.png" alt="max-flow illustration" /></p>
<p>Running max-flow on this graph would determine how to maximize the number of
processed samples. And the actual-values achieved the <code>?</code> arrows correspond to
a best assignment.</p>
<p>This is all good but in real-life we need to consider multiple days and other
constraints. The iterated aspects may be annoying to model but somewhat
manageable. For instance, we could add staged sources and sinks where some
capacity connects a laboratory at two time intervals (this way <em>leftovers</em> can
be post-poned as additional inputs).  In the following picutre, we add a suffix
<code>_0</code> and <code>_1</code> to represent the different stages. Thus an arrow <code>lab1_0 -&gt; lab1_1</code> represents leftovers at <code>lab1</code> from the first staged processed in the
second stage. We could view these arrows accross stages as a degenerate form of
sending samples between two different places in the time dimension.</p>
<p><img src="/gen/images/labspread-maxflow-iterated.dot.png" alt="max-flow illustration" /></p>
<p>Such a model starts to be slightly more cumbersome than the previous one, and
also is <em>wrong</em> because we do not properly model the deadline component.  For
instance, with many stages we could have a long chain of leftover samples until
all samples are processed, irrespectively from how old the samples are.
Instead of having <code>leftovers</code> from labs to labs at future time-steps, we rather
need the <code>leftover</code> arrows to go from sampling location to lab at future
time-steps. Correcting for this fact we get the following schema:</p>
<p><img src="/gen/images/labspread-maxflow-iterated2.dot.png" alt="max-flow illustration" /></p>
<p>This model now seems correct and useful. However, in real-world setups we often
refine models as new constraints are discovered or as new decisions are
required.  For instance, one behavior that would significantly alter the model
would be to limit sampling sites to send samples to at most two labs.  We may
have to scrap our solution becaose the max-flow algorithm is not “cut” to make
choices between alternatives (what we need is a form of knapsack at each
sample-collection location). Such situations are common, and they correspond to
situations where constraint-programming modeling shines.
Constraint programming will let you trade generality for performance (i.e., we
expect a slow down by using a more-general approach). Other situations where
the adaptability of constraint-programming is useful is to probe what happens
when a constraint actually turns-out to be a secondary goal (i.e., we relax a
hard constraint into a soft-constraint and pretend we are geniuses).</p>
<p>Enough introduction, let’s see how we could formulate the problem in a
constraint-programming language (using <a href="https://minizinc.org/">MiniZinc</a> as
usual on this blog).</p>
<h1>A constraint-programming model</h1>
<p>Remember that to model a constraint-programming problem we need to formulate
whate are <em>inputs</em>, <em>decisions</em>, and <em>constraints</em>. We also need some form of
<em>objective</em>.</p>
<h2>high-level model</h2>
<p>We will assume that we have some time-based model of the demand in number of
tests. We also assume that we know the labs capacities (this is no different
from our introductory model).</p>
<p>In our case, a good starting point would be:</p>
<p><strong>inputs</strong></p>
<ul>
<li>demand for samples from sample-collection locations
</li>
<li>PCR lab capacities
</li>
<li>some measure of the time it takes to send samples from one testing-site to a lab
</li>
<li>the duration for which a sample is valid
</li>
</ul>
<p><strong>decisions</strong></p>
<ul>
<li>some assignment from testing-sites to labs
</li>
</ul>
<p><strong>constraints</strong></p>
<ul>
<li>samples must be processed within their lifetime or will be wasted
</li>
<li>labs total work cannot overshoot their capacity
</li>
<li>some form of <strong>conservation law</strong> to say that samples are either tested in time or thrown away
</li>
</ul>
<p><strong>objective</strong></p>
<ul>
<li>minimize the number of samples thrown away otherwise you can always assign zero from any testing-site to any lab
</li>
</ul>
<h2>formalised in MiniZinc</h2>
<p>The core of the model would be something as follows:</p>
<pre><code>int: nLab;
int: nZone;
int: nTime;
int: sample_lifetime;

set of int: LAB = 1..nLab;
set of int: ZONE = 1..nZone;
set of int: TIME = 1..nTime;
set of int: DELAY = 0..sample_lifetime;

% demand of samples produced by zones
array[ZONE,TIME] of int: demand;
% capacity of labs to process samples, may vary with time to model things like week-ends
array[LAB] of int: capacity;
% transit represents a delay from one zone to some lab
array[ZONE,LAB] of int: transit;


% decisions about routing samples: dispatching and dropping samples
int: maxAttr = sum(z in ZONE, t in TIME)(demand[z,t]);
array[ZONE,LAB,TIME,DELAY] of var 0..maxAttr: dispatch;
array[ZONE,TIME] of var 0..maxAttr: dropped;
</code></pre>
<p>Which introduces the inputs and the key decisions. We name <code>ZONE</code> the set of
sampling-locations because we could aggregate locations into zones beforehand,
but also because we I found it hard to juggle with <code>LAB</code> and <code>LOC</code> all the time in
my mind.</p>
<p>More importantly, we refined the decision to explicitly add a <code>DELAY</code> component
to the <code>dispatch</code>-ed amount from a given ZONE to a given LAB at a given TIME.
This delay corresponds to the dashed arrows we introduced in the max-flow model
before. This extra breakdown is not especially surprising because we need to
model the same “allowed physical behavior” whatever modeling technique we use.
That said, in this model we went a bit further and we have broken down the
total delay into two components: transit times (which are imposed by the
<code>transit</code> matrix) and queueing time (i.e., if a lab is busy one day but as
spare capacity the next day we can backlog). The framing is very similar but
are being a bit more explicit which chunk of the delay is forced upon us versus
what chunk of the delay is a proper decision: this break down is important if
you were to iterate on the model, or decide to increase capacity.</p>
<p>We also have a separate explicit <code>dropped</code> decision, for all the samples that
we cannot process. This decision is new compared to the max-flow model because
in the max-flow model dropped quantities are implicit: dropped quantities
correspond to amount of demand flow under the attributed capacity. In
constraint-programming it is somewhat required to be explicit otherwise the
solver will typically find an “uninteresting” solution (e.g., deciding that we
drop everything implicitly because we have no way to express the conservation
law).</p>
<p>We still need to link everything with constraints and write the objective function.</p>
<h3>conservation law</h3>
<p>The conservation-law is not too complicated: at any given TIME, the demand is
partitioned in two sets: dropped quantities and dispatched quantities.</p>
<pre><code>% must dispatch all demand
constraint forall(z in ZONE, t in TIME)
  (demand[z,t] - dropped[z,t] = sum(l in LAB, d in DELAY)( dispatch[z,l,t,d] ));
</code></pre>
<p>Such conservation laws are often “hidden” behind inequalities. In this case I
initially had a inequality saying that the dispatched amound had to be below
the demand, it took me some time to realize that what is leftover deserves a
concept in its own. Then, the conservation law appeared and it became easier to
reason and debug with this extra ‘dropped’ variable.</p>
<h3>samples must reach a lab and be processed within their lifetime</h3>
<p>Here we are using some <em>implication</em> connective that says that if the transit
time and the considered queueing delay exceeds the sample lifetime, then the
dispatched amount for this zone, lab, and delay has to be zero.</p>
<pre><code>% cannot delay (including transit time) past sample_lifetime
constraint forall(z in ZONE, l in LAB, t in TIME, d in DELAY)
  ((transit[z,l] + d &gt; sample_lifetime) -&gt; dispatch[z,l,t,d] = 0);
</code></pre>
<p>This formal statement is worth reading at a slow pace because the formalism is
not really a natural way of thinking. What makes such statements hard to parse
is that we initially said that we want samples to be processed within their
lifetime (i.e., we want a desired outcome to happen) whereas the statement
speaks about preventing dispatching (i.e., we prevent the inverse of the
desired outcome). I do not claim that it is impossible to reverse the logic,
but I found no trivial ways to do so without introducing new variables.</p>
<p>Combined with the conservation law, this constraint will force undispatcheable
quantities to move to the dropped quantity.</p>
<h3>load versus capacity at LAB</h3>
<p>We need some way to prevent LABs overload. For this we need a definition of
load of a LAB at a given time. This load consists of the dispatched amounts
(including transit costs and backlog-delays).</p>
<pre><code>% helper: workload processed by a lab at a given time (includes delayed dispatches)
array[LAB,TIME] of var int: load;
constraint forall(l in LAB, t_load in TIME)
 ( load[l,t_load] = sum(t_route in t_load-sample_lifetime..t_load where t_route &gt; 0, z in ZONE)
                       (dispatch[z,l,t_route,t_load-t_route])
 );
</code></pre>
<p>It takes some work to sum all the right terms, but once we have the load at a
given time, the constraint becomes trivial.</p>
<pre><code>% cannot overload labs
constraint forall(l in LAB, t in TIME)
  (capacity[l] &gt;= load[l,t]);
</code></pre>
<h3>objective: no waste</h3>
<p>This probably is the easiest part of the work. We sum the total dropped amounts and minimize this.</p>
<pre><code>var int: total_drop = sum(array1d(dropped));

solve minimize total_drop;
</code></pre>
<p>Overall, this objective combined with the conservation law gives some intuition
about what a performant heuristic could look like: starting by dispatching zero
we try to attribute fill labs with from closest zones in total delay.
Fortunately, we do not need to write a heuristic and can just run our MiniZinc
model.</p>
<h2>Running the model</h2>
<p>You will find my model, some example data and even some synthetic-data generator written with a simple Ruby script at <a href="https://github.com/lucasdicioccio/labspread">one of my GitHub repos</a> .</p>
<p>The final model has one modification (see below) but also a slightly different
output format than you get by default with MiniZinc (hopefully it’s more
readable).</p>
<h2>Avenues for modifications</h2>
<p>It’s always important to step back and see how we could change the model. I do
not want to use the word <em>improve</em> the model because we actually build
different models. Indeed: a simple model also requires less input data (which
may not be easy to collect), and less testing/debugging work.</p>
<p>We provide a series of modifications that one may want to apply and discuss how
we could adapt (or not) the model. The easy-changes have snippets of code to
see how the constraint-programming approach is beneficial.</p>
<h3>More time-varying variables</h3>
<p>Lab capacities are given as a fixed value. However we could easily modify the
lab capacities to be known time-varying values. Such a change could allow to
model the effect of opening/closing some labs for WE breaks. The change here is pretty limited, we would introduce a <code>TIME</code> component to the capacity and use it in the adequate constraint:</p>
<pre><code>array[LAB,TIME] of int: capacity;

% cannot overload labs
constraint forall(l in LAB, t in TIME)
  (capacity[l,t] &gt;= load[l,t]);
</code></pre>
<p>Such a model is the one I actually implemented first. The time-varying aspect
of the capacity is not key in the structure of the problem (i.e., we do not
change the number nor the shape of constraints, we merely change the bounds the
load can take). Hence, it did not cost me much more to add.</p>
<h3>More physically-correct model of test-performance</h3>
<p>The quality of tests may deteriorate with the delay: in current model the
‘value’ of a test is implicitly binary: we get either 1 valid sample before the
deadline or 0 past the deadline. We could imagine a different function with a
smoother decrease in performance. Such a behavior could also help in providing
different qualities or different deadlines depending on the sample-collection
sites or the laboratories (e.g., because they use different sampling techniques
or material).</p>
<p>For instance, if we attribute some dispatch value that decreases with the
DELAY, we could use such an objective (turned into a maximization here).</p>
<pre><code>var int: w = sum(z in ZONE, l in LAB, t in TIME, d in DELAY)(dispatch[z,l,t,d] * (sample_lifetime-d));
solve maximize w;
</code></pre>
<h3>Varying processing costs at laboratories</h3>
<p>We can imagine that the network of laboratories may process various samples at
different costs. The cost function could take into account a different price
depending on the LAB. The cost function could even be non-linear with some
base-rate and an overshoot cost (it’s a way to relax moderately the capacity
constraint). Such extra cost-modeling would lead the optimization to fill in
cheapest LABs first in a “waterfilling” approach (starting from the cheapest
LAB reachable from a given ZONE until the LAB is no longer the cheapest
alternative etc.).</p>
<p>We could do something like this:</p>
<pre><code>var int: w = sum(l in LAB, t in TIME)(processing_cost(l, load[l,t]));
solve minimize w + drop_cost * total_drop;
</code></pre>
<p>Given a <code>processing_cost</code> function mapping LABs and loads to a numerical cost
we can incorporate the processing cost into the objective. Care as to be taken
to still take into account the <code>total_drop</code> amount and weight it accordingly to
the processing_cost (otherwise the solver will cynically figure out that
dropping every samples incurs no cost).</p>
<h3>More faithful representation of probabilities</h3>
<p>Actual demand in test is pretty unknown: using stochastic programming could
help having more acceptable results (a typical implementation is to discretize
the probability distribution, another one is to produce a large amount of
simulated scenarios). The change requires to provide a discretized weighed
probability of input demand values instead of one demand value per time step
and average over scenarios. I find hard to stay within the
constraint-programming model framework for this problem because a
constraint-programming model decides once what are the action taken at every
future time step. In such a situation, where you  believe you need to go all
the way to model stochastic behaviors you may as well spend the efforts to
model how successive choices can take advantage of the facts that become known
as the time passes. Here you may want to reach for bigger guns like
<a href="https://www.mit.edu/~dimitrib/Dynamic_Prog_Videos.html">optimal-control and reinforcement learning</a> theory (here, a single-stage CP
could be a rollout-heuristic – but we are eyeing on state-of-the-art topics
rather than on a humble blog post).</p>
<h3>More business constraints</h3>
<p>As provided in the introduction. A reason to avoid spending too much time in
using an off-the-shelf algorithm can be a simple business-rule such as “a
testing site can ship at most twice per day”, which would mean our optimization
no-longer looks like a well-studied <em>max-flow</em> problem. Rather, this real-world
problem becomes flavored with a bit of binpacking because we cannot pick all
the variables of the output attribution matrix independently from each other.</p>
<p>Building such a constraint is non-trivial because you need an array of
auxilliary variables telling how many LABs a ZONE serves at a given TIME
(irrespective of the DELAY). Say you get a <code>sent</code> array. Then force the
cardinality of zeros with a <a href="https://www.minizinc.org/doc-2.5.5/en/lib-globals.html#counting-constraints">global counting
constraint</a>.</p>
<p>In that case, switching the solver type may significantly alter performance. It
would be useful to force ourselves to model this problem as a
mixed-integer-linear-programming problem, but that would become rather complicated if you combine these alterations to the initial problem.</p>
</section><section class="main-section"><h1>Closing remarks</h1>
<p>Before starting modeling the problem with a constraint-programming language,
let’s keep in mind that we should always do the work of trying to apply an
off-the-shelf but under-constrained algorithm: sometimes it works (i.e., you
are lucky that the solution actually fits the extra constraints), sometimes you
get a perspective that is useful for the problem (e.g., here we re-used the
idea of <em>leftovers</em>), and at worst you can also use an “under-constrained”
solution to get a approximation or an higher-bound to accept/reject some
alternative (e.g., you have two alternatives situations you could reject one
alternative because we cannot expect an optimal solution to beat the average of
the other alternative).</p>
</section></div><footer class="footing"><div class="social-links"><a href="https://twitter.com/lucasdicioccio">twitter: @lucasdicioccio</a><a href="https://github.com/lucasdicioccio">github: lucasdicioccio</a><a href="https://linkedin.com/in/lucasdicioccio">linkedin: lucasdicioccio</a></div></footer></article></div></body></html>